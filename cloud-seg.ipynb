{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12533387,"sourceType":"datasetVersion","datasetId":7912110}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q segmentation-models-pytorch\n!pip install -q torch\n!pip install -q torchvision\n!pip install -q albumentations\n!pip install -q huggingface_hub\n!pip install -q datasets\n!pip install -q rasterio","metadata":{"_uuid":"c7204ea5-4e93-468a-b39d-524f3968fcd8","_cell_guid":"fa3c220c-1365-45cd-879a-907c40a8a350","trusted":true,"collapsed":false,"scrolled":true,"execution":{"iopub.status.busy":"2025-07-21T15:17:16.477558Z","iopub.execute_input":"2025-07-21T15:17:16.478353Z","iopub.status.idle":"2025-07-21T15:17:38.399979Z","shell.execute_reply.started":"2025-07-21T15:17:16.478320Z","shell.execute_reply":"2025-07-21T15:17:38.399146Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datasets import load_dataset\n\n# Load the metadata for the high-quality subset from the Hugging Face Hub.\n# This is an extremely fast operation as it does not download the image files.\ncloudsen12_hq = load_dataset(\"csaybar/CloudSEN12-high\")\n\n# Print the dataset structure to confirm successful loading\nprint(cloudsen12_hq)","metadata":{"_uuid":"4af5d071-a98e-4b19-a4db-cfafad36cdd7","_cell_guid":"08dc1f8c-5921-429e-a745-f7d0e2e1f960","trusted":true,"collapsed":false,"scrolled":true,"execution":{"iopub.status.busy":"2025-07-21T15:17:46.901441Z","iopub.execute_input":"2025-07-21T15:17:46.902210Z","iopub.status.idle":"2025-07-21T15:17:48.112227Z","shell.execute_reply.started":"2025-07-21T15:17:46.902178Z","shell.execute_reply":"2025-07-21T15:17:48.111460Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install rasterio","metadata":{"_uuid":"d5ee6c13-3edc-4f21-bcae-547774db3f97","_cell_guid":"753632ca-0e87-4df3-a155-bd11143a4d6f","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install segmentation_models_pytorch","metadata":{"_uuid":"815aa43d-8c6e-4faf-8d7d-5d86d08806c6","_cell_guid":"860bd9f0-f952-42f2-8b2b-42072901164f","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport torch\nimport rasterio\nimport numpy as np\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nfrom torch.utils.data import Dataset, DataLoader\nimport segmentation_models_pytorch as smp\nfrom tqdm.notebook import tqdm\nimport matplotlib.pyplot as plt\nfrom datasets import load_dataset\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# --- Configuration ---\n# This dictionary holds all the key parameters for our experiment.\nCONFIG = {\n    \"DEVICE\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n    \"BATCH_SIZE\": 16,\n    \"NUM_WORKERS\": 2,\n    \"EPOCHS\": 25,\n    \"LEARNING_RATE\": 1e-4,\n    \"WEIGHT_DECAY\": 1e-2,\n    # Using the 10-band set recommended for better cloud/snow/surface differentiation.\n    \"BANDS_TO_USE\": ['B01', 'B02', 'B04', 'B05', 'B08', 'B8A', 'B09', 'B10', 'B11', 'B12'],\n    \"ENCODER\": \"mobilenet_v2\", # Efficient and powerful encoder\n}\n\nprint(f\"Using device: {CONFIG['DEVICE']}\")\n\n# --- Data Acquisition ---\n# Load the dataset metadata from the Hugging Face Hub.\n# The `name` parameter is crucial to select the correct data configuration.\nprint(\"Loading dataset metadata from Hugging Face Hub...\")\ncloudsen12_hq = load_dataset(\"ca-saybar/CloudSEN12-high\", trust_remote_code=True)\nprint(\"Dataset metadata loaded successfully.\")\nprint(cloudsen12_hq)\n\n\n# --- Custom PyTorch Dataset for Hugging Face data ---\nclass CloudDataset(Dataset):\n    \"\"\"\n    Custom PyTorch Dataset for the CloudSEN12 dataset loaded from Hugging Face.\n    \"\"\"\n    def __init__(self, hf_dataset, bands, augmentations=None):\n        \"\"\"\n        Args:\n            hf_dataset (datasets.Dataset): The Hugging Face dataset object (e.g., cloudsen12_hq['train']).\n            bands (list): List of Sentinel-2 band names to use.\n            augmentations (A.Compose, optional): Albumentations pipeline.\n        \"\"\"\n        self.dataset = hf_dataset\n        self.bands = bands\n        self.augmentations = augmentations\n\n    def __len__(self):\n        return len(self.dataset)\n\n    def __getitem__(self, idx):\n        item = self.dataset[idx]\n        \n        # --- Load Image ---\n        # Get the paths from the Hugging Face dataset item.\n        s2_l1c_dir = item['s2_l1c_path']\n        label_path = item['manual_hq_path']\n\n        band_paths = [os.path.join(s2_l1c_dir, f'{band}.tif') for band in self.bands]\n        \n        image_stack = []\n        for band_path in band_paths:\n            with rasterio.open(band_path) as src:\n                band_data = src.read(1, out_shape=(256, 256), resampling=rasterio.enums.Resampling.bilinear)\n                image_stack.append(band_data)\n        \n        image = np.stack(image_stack, axis=-1).astype(np.float32)\n        \n        # --- Load and Process Mask ---\n        with rasterio.open(label_path) as src:\n            mask = src.read(1, out_shape=(256, 256), resampling=rasterio.enums.Resampling.nearest)\n            \n        # Class 2 (Thick Cloud) and 3 (Thin Cloud) are considered \"cloud\".\n        binary_mask = np.where((mask == 2) | (mask == 3), 1, 0).astype(np.float32)\n        \n        # --- Preprocessing and Augmentation ---\n        # Normalize by the standard value for Sentinel-2 L1C products\n        image /= 10000.0\n        \n        if self.augmentations:\n            augmented = self.augmentations(image=image, mask=binary_mask)\n            image = augmented['image']\n            mask = augmented['mask']\n        \n        # Ensure mask has a channel dimension for the loss function.\n        # After ToTensorV2, the mask is (H, W). We need (1, H, W).\n        if mask.ndim == 2:\n            mask = mask.unsqueeze(0)\n            \n        return image, mask\n\n# --- Augmentations ---\ntrain_augs = A.Compose([\n    A.HorizontalFlip(p=0.5),\n    A.VerticalFlip(p=0.5),\n    A.RandomRotate90(p=0.5),\n    A.RandomBrightnessContrast(p=0.2),\n    ToTensorV2()\n])\n\nval_augs = A.Compose([\n    ToTensorV2()\n])\n\n# --- Datasets and DataLoaders ---\nprint(\"Setting up datasets and dataloaders...\")\ntrain_dataset = CloudDataset(cloudsen12_hq['train'], bands=CONFIG['BANDS_TO_USE'], augmentations=train_augs)\nval_dataset = CloudDataset(cloudsen12_hq['validation'], bands=CONFIG['BANDS_TO_USE'], augmentations=val_augs)\n\ntrain_loader = DataLoader(train_dataset, batch_size=CONFIG['BATCH_SIZE'], shuffle=True, num_workers=CONFIG['NUM_WORKERS'], pin_memory=True)\nval_loader = DataLoader(val_dataset, batch_size=CONFIG['BATCH_SIZE'], shuffle=False, num_workers=CONFIG['NUM_WORKERS'], pin_memory=True)\n\nprint(f\"Train dataset size: {len(train_dataset)}\")\nprint(f\"Validation dataset size: {len(val_dataset)}\")\n\n\n# --- Loss Function ---\nclass DiceBCELoss(nn.Module):\n    \"\"\"Combined Dice and BCE loss for robust segmentation.\"\"\"\n    def __init__(self, weight=None, size_average=True):\n        super(DiceBCELoss, self).__init__()\n\n    def forward(self, inputs, targets, smooth=1):\n        # inputs are raw logits from the model\n        bce_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='mean')\n        \n        inputs_prob = torch.sigmoid(inputs)\n        inputs_prob = inputs_prob.view(-1)\n        targets = targets.view(-1)\n        \n        intersection = (inputs_prob * targets).sum()\n        dice_loss = 1 - (2. * intersection + smooth) / (inputs_prob.sum() + targets.sum() + smooth)\n        \n        return bce_loss + dice_loss\n\n# --- Model, Loss, and Optimizer ---\nprint(\"Initializing model...\")\nmodel = smp.Unet(\n    encoder_name=CONFIG['ENCODER'],\n    encoder_weights=\"imagenet\",\n    in_channels=len(CONFIG['BANDS_TO_USE']),\n    classes=1,\n    activation=None # Output raw logits for use with BCEWithLogitsLoss\n)\nmodel.to(CONFIG['DEVICE'])\n\nloss_fn = DiceBCELoss()\noptimizer = torch.optim.AdamW(model.parameters(), lr=CONFIG['LEARNING_RATE'], weight_decay=CONFIG['WEIGHT_DECAY'])\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.5, verbose=True)\n\n# --- Training Loop ---\nbest_val_iou = 0.0\nhistory = {'train_loss': [], 'val_loss': [], 'val_iou': []}\n\nfor epoch in range(CONFIG['EPOCHS']):\n    print(f\"--- Epoch {epoch+1}/{CONFIG['EPOCHS']} ---\")\n    \n    # Training Phase\n    model.train()\n    train_loss = 0.0\n    for images, masks in tqdm(train_loader, desc=\"Training\"):\n        images, masks = images.to(CONFIG['DEVICE']), masks.to(CONFIG['DEVICE'])\n        \n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = loss_fn(outputs, masks)\n        loss.backward()\n        optimizer.step()\n        \n        train_loss += loss.item()\n        \n    avg_train_loss = train_loss / len(train_loader)\n    history['train_loss'].append(avg_train_loss)\n    \n    # Validation Phase\n    model.eval()\n    val_loss = 0.0\n    total_iou = 0.0\n    with torch.no_grad():\n        for images, masks in tqdm(val_loader, desc=\"Validating\"):\n            images, masks = images.to(CONFIG['DEVICE']), masks.to(CONFIG['DEVICE'])\n            outputs = model(images)\n            loss = loss_fn(outputs, masks)\n            val_loss += loss.item()\n            \n            preds = torch.sigmoid(outputs) > 0.5\n            tp, fp, fn, tn = smp.metrics.get_stats(preds.long(), masks.long(), mode='binary')\n            iou = smp.metrics.iou_score(tp, fp, fn, tn, reduction='micro')\n            total_iou += iou.item()\n            \n    avg_val_loss = val_loss / len(val_loader)\n    avg_iou = total_iou / len(val_loader)\n    history['val_loss'].append(avg_val_loss)\n    history['val_iou'].append(avg_iou)\n    \n    scheduler.step(avg_val_loss)\n    \n    print(f\"Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | Val IoU: {avg_iou:.4f}\")\n    \n    if avg_iou > best_val_iou:\n        best_val_iou = avg_iou\n        torch.save(model.state_dict(), 'best_cloud_model.pth')\n        print(f\"Model saved with new best IoU: {best_val_iou:.4f}\")\n\nprint(\"\\n--- Training Finished ---\")\n\n# --- Visualization ---\ndef visualize_predictions(dataset, model, num_samples=5):\n    \"\"\"Plots the input image, true mask, and predicted mask.\"\"\"\n    if not os.path.exists('best_cloud_model.pth'):\n        print(\"No model file found ('best_cloud_model.pth'). Skipping visualization.\")\n        return\n        \n    model.load_state_dict(torch.load('best_cloud_model.pth'))\n    model.to(CONFIG['DEVICE'])\n    model.eval()\n    \n    fig, axes = plt.subplots(num_samples, 3, figsize=(15, num_samples * 5))\n    fig.suptitle(\"Model Predictions vs. Ground Truth\", fontsize=20)\n    \n    for i in range(num_samples):\n        idx = np.random.randint(0, len(dataset))\n        image, mask = dataset[idx] # Tensors are (C, H, W)\n        \n        with torch.no_grad():\n            input_tensor = image.unsqueeze(0).to(CONFIG['DEVICE'])\n            pred_logits = model(input_tensor)\n            pred_mask = (torch.sigmoid(pred_logits) > 0.5).float().cpu().squeeze(0)\n            \n        # Create a false-color composite for visualization (SWIR-NIR-Red)\n        # Band indices in our 10-band list: B11 (8), B08 (4), B04 (2)\n        img_vis_bands = image.numpy()[[8, 4, 2], :, :]\n        # Transpose from (C, H, W) to (H, W, C) for plotting\n        img_vis = img_vis_bands.transpose(1, 2, 0)\n        \n        # Clip and scale for better visualization\n        img_vis = np.clip(img_vis, 0, 0.3) / 0.3\n        \n        true_mask = mask.squeeze().numpy()\n        \n        axes[i, 0].imshow(img_vis)\n        axes[i, 0].set_title(f\"Input Image (False Color) #{idx}\")\n        axes[i, 0].axis('off')\n        \n        axes[i, 1].imshow(true_mask, cmap='gray')\n        axes[i, 1].set_title(\"Ground Truth Mask\")\n        axes[i, 1].axis('off')\n        \n        axes[i, 2].imshow(pred_mask.squeeze(), cmap='gray')\n        axes[i, 2].set_title(\"Predicted Mask\")\n        axes[i, 2].axis('off')\n        \n    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n    plt.show()\n\n# Visualize some results from the validation set\nvisualize_predictions(val_dataset, model)","metadata":{"_uuid":"29c029aa-607f-4bf5-89ce-fbf8b9ff25e6","_cell_guid":"4db25f96-2312-4332-82fa-36d8c88ee941","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-21T15:43:38.700366Z","iopub.execute_input":"2025-07-21T15:43:38.700641Z","iopub.status.idle":"2025-07-21T15:43:38.838979Z","shell.execute_reply.started":"2025-07-21T15:43:38.700625Z","shell.execute_reply":"2025-07-21T15:43:38.838077Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stdout","text":"Using device: cuda\nLoading dataset metadata from Hugging Face Hub...\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mDatasetNotFoundError\u001b[0m                      Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/1016299191.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m# The `name` parameter is crucial to select the correct data configuration.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading dataset metadata from Hugging Face Hub...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0mcloudsen12_hq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ca-saybar/CloudSEN12-high\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrust_remote_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Dataset metadata loaded successfully.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcloudsen12_hq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001b[0m\n\u001b[1;32m   2060\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2061\u001b[0m     \u001b[0;31m# Create a dataset builder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2062\u001b[0;31m     builder_instance = load_dataset_builder(\n\u001b[0m\u001b[1;32m   2063\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2064\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mload_dataset_builder\u001b[0;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, storage_options, trust_remote_code, _require_default_config_name, **config_kwargs)\u001b[0m\n\u001b[1;32m   1780\u001b[0m         \u001b[0mdownload_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdownload_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdownload_config\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mDownloadConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1781\u001b[0m         \u001b[0mdownload_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1782\u001b[0;31m     dataset_module = dataset_module_factory(\n\u001b[0m\u001b[1;32m   1783\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1784\u001b[0m         \u001b[0mrevision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrevision\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, cache_dir, trust_remote_code, _require_default_config_name, _require_custom_configs, **download_kwargs)\u001b[0m\n\u001b[1;32m   1650\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Couldn't reach the Hugging Face Hub for dataset '{path}': {e1}\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1651\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mDataFilesNotFoundError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDatasetNotFoundError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEmptyDatasetError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1652\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0me1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1653\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1654\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mtrust_remote_code\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, cache_dir, trust_remote_code, _require_default_config_name, _require_custom_configs, **download_kwargs)\u001b[0m\n\u001b[1;32m   1576\u001b[0m                 ) from e\n\u001b[1;32m   1577\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mRepositoryNotFoundError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1578\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mDatasetNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Dataset '{path}' doesn't exist on the Hub or cannot be accessed.\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1579\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1580\u001b[0m                 dataset_script_path = api.hf_hub_download(\n","\u001b[0;31mDatasetNotFoundError\u001b[0m: Dataset 'ca-saybar/CloudSEN12-high' doesn't exist on the Hub or cannot be accessed."],"ename":"DatasetNotFoundError","evalue":"Dataset 'ca-saybar/CloudSEN12-high' doesn't exist on the Hub or cannot be accessed.","output_type":"error"}],"execution_count":18},{"cell_type":"code","source":"print(cloudsen12_hq['train'].features)\n","metadata":{"_uuid":"b8a668c0-fe05-405d-9d83-f9117d7e7863","_cell_guid":"05c9c300-0d02-4e80-bc87-9deeef377010","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-07-21T15:25:29.171846Z","iopub.execute_input":"2025-07-21T15:25:29.172158Z","iopub.status.idle":"2025-07-21T15:25:29.178717Z","shell.execute_reply.started":"2025-07-21T15:25:29.172125Z","shell.execute_reply":"2025-07-21T15:25:29.177939Z"}},"outputs":[{"name":"stdout","text":"{'index': Value(dtype='int64', id=None), 'annotator_name': Value(dtype='string', id=None), 'roi_id': Value(dtype='string', id=None), 's2_id_gee': Value(dtype='string', id=None), 's2_id': Value(dtype='string', id=None), 's2_date': Value(dtype='string', id=None), 's2_sen2cor_version': Value(dtype='string', id=None), 's2_fmask_version': Value(dtype='string', id=None), 's2_cloudless_version': Value(dtype='string', id=None), 's2_reflectance_conversion_correction': Value(dtype='float64', id=None), 's2_aot_retrieval_accuracy': Value(dtype='int64', id=None), 's2_water_vapour_retrieval_accuracy': Value(dtype='int64', id=None), 's2_view_off_nadir': Value(dtype='int64', id=None), 's2_view_sun_azimuth': Value(dtype='float64', id=None), 's2_view_sun_elevation': Value(dtype='float64', id=None), 's1_id': Value(dtype='string', id=None), 's1_date': Value(dtype='string', id=None), 's1_grd_post_processing_software_name': Value(dtype='string', id=None), 's1_grd_post_processing_software_version': Value(dtype='float64', id=None), 's1_slc_processing_facility_name': Value(dtype='string', id=None), 's1_slc_processing_software_version': Value(dtype='float64', id=None), 's1_radar_coverage': Value(dtype='float64', id=None), 'land_cover': Value(dtype='int64', id=None), 'label_type': Value(dtype='string', id=None), 'cloud_coverage': Value(dtype='string', id=None), 'test': Value(dtype='int64', id=None), 'difficulty': Value(dtype='float64', id=None), 'proj_epsg': Value(dtype='int64', id=None), 'proj_geometry': Value(dtype='string', id=None), 'proj_shape': Value(dtype='int64', id=None), 'proj_centroid': Value(dtype='string', id=None), 'proj_transform': Value(dtype='string', id=None), 'test_sr': Value(dtype='float64', id=None)}\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"import os\nimport shutil\n\n# The standard cache directory for Hugging Face datasets\ncache_dir = os.path.expanduser(\"~/.cache/huggingface/datasets\")\n\n# Check if the directory exists and remove it completely\nif os.path.exists(cache_dir):\n    print(f\"Found cache directory: {cache_dir}\")\n    print(\"Removing it now...\")\n    shutil.rmtree(cache_dir)\n    print(\"Cache cleared successfully. âœ…\")\nelse:\n    print(\"Cache directory not found, no action needed.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T15:51:44.697311Z","iopub.execute_input":"2025-07-21T15:51:44.697867Z","iopub.status.idle":"2025-07-21T15:51:44.705368Z","shell.execute_reply.started":"2025-07-21T15:51:44.697842Z","shell.execute_reply":"2025-07-21T15:51:44.704499Z"}},"outputs":[{"name":"stdout","text":"Found cache directory: /root/.cache/huggingface/datasets\nRemoving it now...\nCache cleared successfully. âœ…\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"from datasets import load_dataset\n\nprint(\"Attempting to load the dataset from the Hub...\")\n\n# This should now work by forcing a fresh download\ncloudsen12_hq = load_dataset(\"ca-saybar/CloudSEN12-high\", trust_remote_code=True)\n\nprint(\"Dataset loaded successfully! ðŸŽ‰\")\nprint(cloudsen12_hq)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T15:51:54.180282Z","iopub.execute_input":"2025-07-21T15:51:54.180827Z","iopub.status.idle":"2025-07-21T15:51:54.330968Z","shell.execute_reply.started":"2025-07-21T15:51:54.180804Z","shell.execute_reply":"2025-07-21T15:51:54.330015Z"}},"outputs":[{"name":"stdout","text":"Attempting to load the dataset from the Hub...\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mDatasetNotFoundError\u001b[0m                      Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/3937453615.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# This should now work by forcing a fresh download\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mcloudsen12_hq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ca-saybar/CloudSEN12-high\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrust_remote_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Dataset loaded successfully! ðŸŽ‰\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001b[0m\n\u001b[1;32m   2060\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2061\u001b[0m     \u001b[0;31m# Create a dataset builder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2062\u001b[0;31m     builder_instance = load_dataset_builder(\n\u001b[0m\u001b[1;32m   2063\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2064\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mload_dataset_builder\u001b[0;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, storage_options, trust_remote_code, _require_default_config_name, **config_kwargs)\u001b[0m\n\u001b[1;32m   1780\u001b[0m         \u001b[0mdownload_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdownload_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdownload_config\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mDownloadConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1781\u001b[0m         \u001b[0mdownload_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1782\u001b[0;31m     dataset_module = dataset_module_factory(\n\u001b[0m\u001b[1;32m   1783\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1784\u001b[0m         \u001b[0mrevision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrevision\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, cache_dir, trust_remote_code, _require_default_config_name, _require_custom_configs, **download_kwargs)\u001b[0m\n\u001b[1;32m   1650\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Couldn't reach the Hugging Face Hub for dataset '{path}': {e1}\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1651\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mDataFilesNotFoundError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDatasetNotFoundError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEmptyDatasetError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1652\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0me1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1653\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1654\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mtrust_remote_code\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, cache_dir, trust_remote_code, _require_default_config_name, _require_custom_configs, **download_kwargs)\u001b[0m\n\u001b[1;32m   1576\u001b[0m                 ) from e\n\u001b[1;32m   1577\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mRepositoryNotFoundError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1578\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mDatasetNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Dataset '{path}' doesn't exist on the Hub or cannot be accessed.\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1579\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1580\u001b[0m                 dataset_script_path = api.hf_hub_download(\n","\u001b[0;31mDatasetNotFoundError\u001b[0m: Dataset 'ca-saybar/CloudSEN12-high' doesn't exist on the Hub or cannot be accessed."],"ename":"DatasetNotFoundError","evalue":"Dataset 'ca-saybar/CloudSEN12-high' doesn't exist on the Hub or cannot be accessed.","output_type":"error"}],"execution_count":21},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}